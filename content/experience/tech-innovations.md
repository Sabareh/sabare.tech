---
company: "Stanbic Bank Kenya"
position: "Data Engineer"
location: "Nairobi County, Kenya"
startDate: "2025-06-01"
endDate: null
current: true
type: "full-time"
logo: "/companies/stanbic-bank-logo.png"
website: "https://www.standardbank.com/kenya"
description: "Modernizing legacy Oracle/EDW workloads into observable Python + Airflow pipelines with validation-first data contracts."
team: 12
technologies:
  - "Python"
  - "Apache Airflow"
  - "Polars"
  - "Oracle"
  - "SQL"
  - "Delta Lake"
  - "Kafka"
  - "dbt"
achievements:
  - title: "Oracle to Airflow Modernization"
    description: "Re-platformed GL integrity, interest change, and EDW ingestion workloads into Python + Airflow DAGs"
    impact: "Reduced operational toil and enabled auditable deployments"
    metrics:
      - "8+ legacy workflows modernized"
      - "35% faster release cycles"
      - "Zero unplanned downtime post cut-over"
  - title: "Validation-First Data Contracts"
    description: "Embedded validation checkpoints, alerting, and rollback playbooks across critical pipelines"
    impact: "Protects regulatory and stakeholder reporting for Finance, Risk, and Operations"
    metrics:
      - "25+ SLAs governed"
      - "100% automated rollback coverage"
      - "Real-time Power BI observability"
  - title: "Stakeholder Enablement"
    description: "Published Excel runbooks, metrics, and dashboards so teams can self-serve pipeline health"
    impact: "Accelerated decisions and reduced support escalations"
    metrics:
      - "15 dashboards shipped"
      - "30% faster issue triage"
      - "High adoption across Finance/Risk/Ops"
projects:
  - "GL Integrity Modernization"
  - "Interest Change DR/CR Automation"
  - "Enterprise Data Warehouse Ingestion Playbooks"
---

# Stanbic Bank Kenya - Data Engineer

## Mission

Modernize legacy Oracle/EDW workloads into resilient Python + Airflow pipelines that are fully observable, validation-first, and easy for operations to support.

## What I Own

### Data Contract Design

- Translate requirements from Finance, Risk, and Operations into versioned schemas, SLAs, and validation rules
- Maintain reusable query → read_sql → to_sql patterns for ingestion and curation layers
- Embed pre/post checks, automated rollbacks, and alerting into every DAG

### Pipeline Engineering

- Build batch and streaming workflows with Airflow, Polars, Delta Lake, and Kafka
- Harden legacy processes with modular Python code, environment-aware configs, and automated tests
- Ensure high availability by instrumenting metrics, logs, and dashboards consumed in Power BI

### Stakeholder Enablement

- Deliver Excel runbooks, observability dashboards, and weekly readouts to reduce escalations
- Partner with platform teams to align on naming conventions, security posture, and deployment playbooks
- Lead cross-functional workshops so teams understand data lineage and recovery paths

## Highlights

- **Zero-regret cut-over** from Oracle cron jobs to Airflow DAGs, eliminating manual interventions and opaque deployments
- **Validation-first mindset** with more than 25 SLAs protected by automated checks and alerting
- **Faster delivery cadence** by introducing reusable ingestion templates and shared code modules

## Tooling Stack

- Python, Polars, Pandas
- Apache Airflow, dbt, Delta Lake
- Oracle EDW, SQL Server, Kafka
- Power BI, Excel runbooks, Microsoft Fabric
